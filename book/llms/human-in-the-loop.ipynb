{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd34523",
   "metadata": {},
   "source": [
    "(llms-human-in-the-loop)=\n",
    "# The Importance of Human-in-the-Loop\n",
    "\n",
    "While AI coding assistants can significantly speed up development, it is important to remember that these tools are meant to assist, not replace, human programmers. AI models can make critical errors, often in unexpected ways. To maintain high-quality code, it is essential to have **human oversight** by reviewing, testing, and validating any AI-generated code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccd9d9b",
   "metadata": {},
   "source": [
    "## Hallucinations\n",
    "\n",
    "AI models are more likely to produce incorrect outputs than admit to not knowing how to answer a question. Often, they will be confident in their wrong answers, which can lead to significant issues if not caught early. This phenomenon is known as **hallucination**.\n",
    "\n",
    "Real-world examples of AI hallucinations are numerous (and humorous), ranging from news articles publishing summer reading lists of fake books to attorneys referring to non-existent cases in court. \n",
    "\n",
    "In coding, this often means that the AI-generated code does not compile. In those cases, it is easy to spot the error. However, bugs, logic errors, or security vulnerabilities can be much harder to detect. These issues can lead to significant problems in production systems, including data loss, security breaches, and system failures.\n",
    "\n",
    "```{admonition} Example\n",
    ":class: example\n",
    "A famous example of LLMs being wrong is asking them to count. The rule-based nature of counting is not compatible with the probabilistic nature of LLMs. Try asking how many characters are in the following text:\n",
    "> This is an example test designed to fool LLMs.\n",
    "The correct answer is 46. You will likely get a different answer each time you ask, and it will almost always be wrong.\n",
    "```\n",
    "\n",
    "In smaller Python scripts, it may simply mean that the code does not do what you expect it to do. \n",
    "\n",
    "The nondeterministic nature of LLMs means that they can generate different outputs for the same input, which can lead to inconsistencies and errors. This is particularly problematic in coding, where precision is crucial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf0215",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "To mitigate the risks associated with AI coding assistants, it is essential to follow best practices:\n",
    "\n",
    "- **Review AI-generated code**: Always review the code generated by AI assistants. Look for logical errors, security vulnerabilities, and other potential issues.\n",
    "- **Do not copy paste large blocks of code**: Instead, use AI to generate small snippets or to help with specific tasks. This allows you to maintain control over the code and reduces the risk of introducing errors.\n",
    "- **Make sure you always understand the code**: If you do not understand what the AI-generated code does, it is likely that you will not be able to spot errors or issues. Take the time to read and understand the code before using it.\n",
    "- **Test thoroughly**: Run tests on the AI-generated code to ensure it works as expected."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
